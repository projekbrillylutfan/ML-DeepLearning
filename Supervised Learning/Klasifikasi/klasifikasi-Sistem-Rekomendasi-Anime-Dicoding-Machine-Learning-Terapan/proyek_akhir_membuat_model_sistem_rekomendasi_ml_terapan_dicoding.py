# -*- coding: utf-8 -*-
"""Proyek Akhir : Membuat Model Sistem Rekomendasi ml terapan dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pmbEQpTMR0sP-PGEr-xnFj1_NbQdrNP

# Proyek Akhir Submission (Sistem Rekomendasi Anime)
Disusun oleh : Brilly Lutfan Qasthari
Laporan tentang hasil dokumentasi submission terakhir bertema Sistem Rekomendasi menggunakan *Content-based Filtering* dan *Collaborative Filtering*.
## Project Overview

Dalam beberapa tahun terakhir, anime telah menjadi fenomena global dan semakin populer di kalangan pecinta hiburan. Dengan peningkatan popularitas ini, jumlah anime yang bisa ditonton juga meningkat pesat. Namun, dengan banyaknya pilihan anime, pengguna seringkali kesulitan menemukan anime yang sesuai dengan minat dan preferensi mereka. Hal ini dapat membuat pengguna bingung, membuang waktu, bahkan frustasi dalam mencari anime yang diinginkan. 

Dalam konteks ini, sistem rekomendasi anime sangatlah penting. Sistem ini dapat membantu pengguna menemukan anime baru yang sesuai dengan minat, mengurangi kebingungan dalam memilih, dan memperluas wawasan anime. Dengan menggunakan teknik dan algoritma yang tepat, sistem rekomendasi anime dapat memberikan rekomendasi yang dipersonalisasi dan relevan kepada pengguna, sehingga meningkatkan pengalaman menonton dan kepuasan pengguna [[1](https://ojs.unud.ac.id/index.php/jnatia/article/download/92636/47032/)]. 

Sistem rekomendasi anime adalah sistem yang merekomendasikan anime kepada pengguna berdasarkan preferensi mereka. Sistem ini dapat membantu pengguna menemukan anime yang sesuai dengan selera. Latar belakang masalah yang dihadapi dalam pengembangan sistem rekomendasi anime adalah kurangnya kualitas informasi yang baik dan keterbatasan jumlah informasi [[2](https://repository.upnvj.ac.id/2016/1/AWAL.pdf)]. 

Oleh karena itu, pengembangan sistem rekomendasi anime sangat menarik untuk diteliti lebih lanjut. Sistem rekomendasi menawarkan berbagai manfaat yang dapat dicapai baik oleh pengguna maupun pengusaha. Oleh karena itu, sangat penting untuk dikembangkan lebih lanjut, terutama di saat informasi dan data berkembang dengan sangat cepat. Dalam proyek ini saya juga fokus pada pengembangan sistem rekomendasi anime.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

!git clone https://github.com/projekbrillylutfan/kumpulan_dataset

anime_ovrl = pd.read_csv("/content/kumpulan_dataset/Anime Dataset Rekomen/anime.csv")
rtg_anime = pd.read_csv("/content/kumpulan_dataset/Anime Dataset Rekomen/rating.csv")

"""## Business Understanding

Dalam bisnis, sistem rekomendasi anime memiliki potensi besar untuk meningkatkan pengalaman pengguna, meningkatkan pangsa pasar, dan meningkatkan keterlibatan pengguna dalam industri anime. Berikut adalah beberapa aspek bisnis dari sistem rekomendasi anime seperti: 

1. **Personalisasi dan Pengalaman Pengguna yang Lebih Baik**
   Dengan menggunakan sistem rekomendasi anime, perusahaan dapat menawarkan pengalaman yang lebih personal dan relevan kepada pengguna. Dengan memahami preferensi pengguna, sistem dapat merekomendasikan anime berdasarkan minat mereka. Hal ini dapat meningkatkan kepuasan pengguna, meningkatkan waktu yang dihabiskan pengguna untuk menonton anime, dan meningkatkan loyalitas pengguna terhadap platform atau layanan yang memberikan rekomendasi tersebut.
2. **Meningkatkan Retensi Pengguna**
   Dalam industri yang penuh dengan pilihan anime, retensi pengguna menjadi faktor kunci kesuksesan bisnis. Sistem rekomendasi anime dapat membantu mempertahankan pengguna dengan terus memberikan rekomendasi yang relevan. Dengan mempertahankan pengguna yang ada, organisasi dapat meningkatkan nilai seumur hidup pengguna dan mengurangi churn. 
3. **Diversifikasi Penonton dan Mempengaruhi Pembelian**
   Sistem rekomendasi anime dapat membantu bisnis mengatasi tantangan dalam menjangkau audiens yang berbeda. Dengan menganalisis preferensi dan perilaku pengguna, sistem dapat merekomendasikan anime yang mungkin tidak diketahui pengguna atau yang berada di luar zona nyaman mereka. Ini dapat membantu perusahaan memperluas audiens mereka, memperkenalkan anime baru kepada pengguna, dan bahkan memengaruhi keputusan pembelian dalam bentuk pesanan barang dagangan, DVD, atau layanan streaming.

### Problem Statements

Berdasarkan masalah yang telah dijelaskan sebelumnya, masalah utama untuk proyek ini adalah sebagai berikut.
- Apakah sistem dapat memberikan rekomendasi yang relevan ?
- Bagaimana performa model dalam memprediksi rekomendasi untuk user ?

### Goals

Adapun tujuan yang dicapai oleh proyek ini sebagai berikut:
- Menghasilkan sebuha model sistem rekomendasi berbasis konten untuk memprediksi rekomendasi pengguna.
- Menghasilkan sebuha model sistem rekomendasi berbasis *collaborative filtering* untuk memprediksi rekomendasi pengguna.

### Solution statements

Adapun solusi yang dapat diberikan untuk proyek ini meliputi:
- Membuat sebuah sistem rekomendasi berbasis *Content-based Filtering* berdasarkan data genre anime.
- Membuat sebuah sistem rekomendasi berbasis *Collaborative Filtering* berdasarkan data rating anime yang telah diberikan oleh user.
"""

anime_ovrl

anime_ovrl.shape

rtg_anime

rtg_anime.shape

print('Jumlah data anime di dalam database : ', len(anime_ovrl.anime_id.unique()))
print('Jumlah data rating anime dalam database: ', len(rtg_anime.user_id.unique()))

print('Jumlah data genre anime dalam database: ', len(anime_ovrl.genre.unique()))
print('Jumlah data tipe anime dalam database: ', len(anime_ovrl.type.unique()))

"""# **Data Understanding**
Dataset yang digunakan dalam proyek ini adalah daftar judul anime dengan fitur, jumlah penggemar, dan rata-rata rating pengguna. Dataset dapat di unduh [disini](https://github.com/projekbrillylutfan/kumpulan_dataset/tree/main/Anime%20Dataset%20Rekomen), Kumpulan data ini berisi informasi data preferensi pengguna dari 73.516 pengguna di 12.294 anime. Setiap pengguna dapat menambahkan anime ke daftar lengkap mereka dan memberikannya peringkat dan kumpulan data ini adalah kompilasi dari peringkat tersebut.

variabel pada data `anime.csv` meliputi :
* `anime_id` = id unik untuk data ini.
* `name` = judul anime secara lengkap.
* `genre` = genre anime secara lengkap.
* `type` = tipe tayang anime.
* `episodes` = jumlah episode anime.
* `rating` = rata-rata rating anime.
* `members` = jumlah komunitas member di setiap anime.

variabel pada data `rating.csv` meliputi :
* `user_id ` = id unik untuk user.
* `anime_id ` = id untuk user yang telah voting.
* `rating ` = rentang rating dari 1 sampai 10.
"""

anime_ovrl.info()

rtg_anime.info()

anime_ovrl.isna().sum()

anime_ovrl.duplicated().sum()

for col in anime_ovrl.select_dtypes(include=[np.number]).columns:
    count = (anime_ovrl[col] == 0).sum()
    print(f"Nilai 0 di kolom {col} ada: {count}")

anime_ovrl.describe()

anime_ovrl = anime_ovrl.dropna()

"""# Data Preprocessing
1. menghapus simbol pada anime.
2. menghapus outlier pada data `anime.csv` dan `rating.csv`.
3. menghapus data duplikat.
4. menghapus data yang memiliki missing value.

## **Missing Value**
"""

anime_ovrl.isna().sum()

anime_ovrl.describe()

for col in anime_ovrl.select_dtypes(exclude=[np.number]).columns:
    kate = anime_ovrl[col].value_counts()
    print(f"Rincian di {col} : ")
    print("----------------------awal--------------------")
    print(kate)
    print("----------------------akhir--------------------")

rtg_anime.isna().sum()

rtg_anime.duplicated().sum()

for col in rtg_anime.select_dtypes(include=[np.number]).columns:
    count = (rtg_anime[col] == 0).sum()
    print(f"Nilai 0 di kolom {col} ada: {count}")

rtg_anime.drop_duplicates(keep='first',inplace=True)

rtg_anime.duplicated().sum()

anime_ovrl

columns = ['anime_id', 'rating', 'members']
for column in columns:
    sns.boxplot(x=anime_ovrl[col])

sns.boxplot(x=anime_ovrl['anime_id'])

sns.boxplot(x=anime_ovrl['members'])

anime_ovrl.shape

Q1 = anime_ovrl.quantile(0.25)
Q3 = anime_ovrl.quantile(0.75)
IQR=Q3-Q1
anime_ovrl=anime_ovrl[~((anime_ovrl<(Q1-1.5*IQR))|(anime_ovrl>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
anime_ovrl.shape

rtg_anime.info()

sns.boxplot(x=rtg_anime['user_id'])

sns.boxplot(x=rtg_anime['anime_id'])

sns.boxplot(x=rtg_anime['rating'])

rtg_anime.shape

Q1 = rtg_anime.quantile(0.25)
Q3 = rtg_anime.quantile(0.75)
IQR=Q3-Q1
rtg_anime=rtg_anime[~((rtg_anime<(Q1-1.5*IQR))|(rtg_anime>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah kita drop outliers
rtg_anime.shape

import re
def text_clean(text):
    text = re.sub(r'"', '', text)
    text = re.sub(r'\.hack//', '', text)
    text = re.sub(r"'", '', text)
    text = re.sub(r"A's", '', text)
    text = re.sub(r"I'", "I'", text)
    text = re.sub(r'&', 'and', text)
    
    return text

anime_ovrl['name'] = anime_ovrl['name'].apply(text_clean)

anime_ovrl

"""# **Univariate Analysis**

Univariate Analysis adalah metode statistik yang digunakan untuk menganalisis satu variabel tunggal dalam suatu dataset. Tujuan dari analisis univariat adalah untuk memahami distribusi, pola, dan karakteristik dari variabel tersebut secara terpisah.

![distribusi anime](https://i.ibb.co/HCpSxvb/distribusi.png)
Gambar 1. Distribusi Anime
Pada Gambar 1 dapat dilihat bahwa data anime memiliki rating anime terendah 1,67 dan rating tertinggi 10 dengan rata-rata 6,48. Dataset ini juga memiliki jumlah anggota komunitas anime terendah yaitu 12 dan tertinggi 1013917 dengan rata-rata 18348. Perbedaan jumlah minimum dan maksimum anggota komunitas anime cukup besar dan itu bisa dimaklumi karena beberapa anime sangat populer dan beberapa tidak.

![distribusi rating](https://i.ibb.co/XzJZfGH/distribusi-rating.png)
Gambar 2. Distribusi Rating

Pada Gambar 2. menjelaskan bahwa Dalam catatan peringkat anime, peringkat terendah yang diberikan pengguna pada anime adalah -1 dan peringkat tertinggi adalah 10. Peringkat -1 berarti pengguna menonton anime tetapi tidak memberi peringkat. Pengguna sampel yang tidak memberikan peringkat tidak akan digunakan dan karenanya akan dihapus.

![Genre Terbanyak](https://i.ibb.co/m9dVbKM/genre.png)
Gambar 3. Visualisasi Genre Terbanyak

Pada Gambar 3 dapat dilihat kalau genre terbanyak ada di kategori hentai, kategori banyak kedua ada di genre komedi dan kategori terbanyak kedua ada di genre musik.
"""

anime_ovrl.describe().apply(lambda s: s.apply('{0:.2f}'.format))

anime_ovrl['genre'].value_counts(normalize=True).head(10)

anime_ovrl['genre'].value_counts().head(10).plot(kind='bar')
plt.title('10 Teratas Genre Anime')
plt.show()

anime_ovrl.groupby('genre')['genre'].agg('count')

rtg_anime.describe().apply(lambda s: s.apply('{0:.2f}'.format))

rtg_anime = rtg_anime[~(rtg_anime.rating == -1)]

print('rating terbanyak oleh 1 user:', max(rtg_anime['user_id'].value_counts().values))
print('rating tersedikit oleh 1 user:', min(rtg_anime['user_id'].value_counts().values))
print('jumlah rating oleh 1 user:', rtg_anime['user_id'].value_counts().median())

print('rating terbanyak untuk 1 anime:', max(rtg_anime['anime_id'].value_counts().values))
print('rating tersedikit untuk 1 anime:', min(rtg_anime['anime_id'].value_counts().values))
print('jumlah rating untuk 1 anime AVG:', rtg_anime['anime_id'].value_counts().median())

sns.histplot(x='rating', data=rtg_anime)

"""# **Data Preparation**

Pada tahapan ini saya melakukan:
1. Encoding kolom user id dan dan anime id yang bertujuan untuk mengubah data dari format yang asli menjadi format yang dapat dipahami atau diproses oleh komputer. Encoding adalah proses representasi data dengan menggunakan aturan tertentu agar dapat disimpan, ditransmisikan, atau diproses dengan efisien dan akurat.
2. Normalisasi nilai rating untuk memudahkan training hal ini bertujuan data untuk mengubahnya menjadi bentuk yang lebih terstandarisasi atau normal. Tujuan normalisasi adalah untuk menghilangkan perbedaan skala, mengurangi bias, atau menyesuaikan data agar dapat dibandingkan atau diproses dengan lebih baik.
3. Train-Test Split untuk melakukan validasi model dengan pembagian 80% data training dan 20% data testing
"""

df_train = rtg_anime.copy()

df_train

user_ids = df_train['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

anime_ids = df_train['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

df_train['user_id'] = df_train['user_id'].map(user_to_user_encoded)
df_train['anime_id'] = df_train['anime_id'].map(anime_to_anime_encoded)

num_users = len(user_to_user_encoded)
num_anime = len(anime_to_anime_encoded)

df_train['rating'] = df_train['rating'].values.astype(np.float32)
min_rating = min(df_train['rating'])
max_rating = max(df_train['rating'])

df_train = df_train.sample(frac=1, random_state=42)
X = df_train[['user_id', 'anime_id']].values
y = df_train['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
train_indices = int(0.8 * df_train.shape[0])

X_train, X_test, y_train, y_test = (X[:train_indices], X[train_indices:], y[:train_indices], y[train_indices:])

"""# **Modelling**

Dalam proyek ini, saya menggunakan dua jenis sistem rekomendasi yaitu *content-based filtering* (CBF) dan *collaborative filtering* (CF). Untuk CBF, saya akan menggunakan vektorisasi TF-IDF dan cosine similarity sebagai similarity function-nya.
Untuk kelebihan vektorisasi TF-IDF adalah :
1. Merepresentasikan kepentingan relatif kata dalam dokumen: Dengan menggunakan skema bobot TF-IDF, vektorisasi TF-IDF memberikan bobot yang lebih tinggi untuk kata-kata yang jarang muncul dalam dokumen tetapi muncul secara khusus dalam dokumen tertentu. Hal ini membantu dalam mengidentifikasi kata-kata kunci atau kata-kata yang paling mewakili dokumen.
2. Mengurangi dampak kata-kata umum: Vektorisasi TF-IDF secara inheren memberikan bobot yang lebih rendah untuk kata-kata umum yang muncul di banyak dokumen. Ini membantu dalam mengidentifikasi kata-kata yang lebih spesifik dan lebih deskriptif dari sebuah dokumen.
3. Efektif dalam mewakili dokumen panjang: Vektorisasi TF-IDF cenderung efektif dalam mengatasi masalah dokumen yang panjang dengan banyak kata. Bobot TF-IDF membantu dalam menyoroti kata-kata kunci yang unik dan penting dalam dokumen.

Untuk kekurangan vektorisasi TF-IDF :
1. Tidak memperhitungkan konteks kata: Vektorisasi TF-IDF hanya memperhitungkan keberadaan kata dalam dokumen dan frekuensi kemunculannya, tanpa memperhitungkan konteks kata. Artinya, informasi tentang urutan kata dan hubungannya tidak diperhitungkan dalam vektorisasi ini.
2. Tidak mempertimbangkan makna kata: Vektorisasi TF-IDF tidak mempertimbangkan makna kata-kata secara langsung. Hal ini dapat menyebabkan masalah jika kata-kata yang serupa secara makna tetapi berbeda secara ortografi diberikan bobot yang berbeda secara signifikan.

Kelebihan cosine similarity: 
1. Mengukur kesamaan semantik: Cosine similarity mengukur kesamaan antara dua vektor dokumen berdasarkan arah dan bukan panjang vektor. Ini memungkinkan pengukuran kesamaan yang lebih baik secara semantik daripada hanya menggunakan frekuensi kata.
2. Skala independen: Cosine similarity tidak tergantung pada skala data atau panjang dokumen. Hal ini memungkinkan perbandingan yang adil antara dokumen-dokumen dengan panjang yang berbeda.

Kekurangan cosine similarity:
1. Tidak memperhitungkan konteks kata: Seperti halnya vektorisasi TF-IDF, cosine similarity juga tidak memperhitungkan konteks kata. Informasi tentang urutan kata dan hubungannya tidak diperhitungkan dalam perhitungan kesamaan.
2. Rentan terhadap kata-kata umum: Cosine similarity tidak secara langsung mengurangi dampak kata-kata umum. Kata-kata yang sering muncul dalam banyak dokumen dapat memiliki bobot yang tinggi dan mempengaruhi hasil perhitungan kesamaan.

Hasil output dari CBF ada di Gambar 4 sebagai berikut:
![Oni rekomendasi](https://i.ibb.co/NjSvjzX/oni-rekomendasi.png)
Gambar 4. Hasil Prediksi CBF
Dari Gambar 4 dapat dilihat kalau sistem ingin merekomendasikan judul "Oni" dengan genre demon, historikal, dan supernatural. Sistem dapat merekomendasikan dengan genre yang tepat dengan sesuai dan presisi.

Lalu untuk *collaborative filtering* (CF) merupakan salah satu metode yang digunakan dalam sistem rekomendasi untuk menyajikan rekomendasi kepada pengguna berdasarkan preferensi atau perilaku pengguna lain yang memiliki kesamaan dengan pengguna tersebut. Metode ini memanfaatkan informasi dari pengguna-pengguna lain atau item-item yang serupa untuk memberikan rekomendasi.

Kelebihan CF:
1. Memperhitungkan preferensi pengguna: Collaborative filtering memperhitungkan preferensi pengguna berdasarkan data riwayat pengguna atau perilaku pengguna lain yang memiliki kesamaan dengan pengguna target. Hal ini memungkinkan rekomendasi yang lebih personal dan relevan dengan preferensi pengguna.
2. Tidak membutuhkan informasi konten: Collaborative filtering tidak membutuhkan informasi rinci tentang item atau konten yang direkomendasikan. Ini memungkinkan sistem rekomendasi untuk memberikan rekomendasi bahkan jika tidak ada informasi konten yang tersedia.
3. Dapat menemukan preferensi baru: Collaborative filtering dapat menemukan preferensi baru yang mungkin tidak terdeteksi oleh metode rekomendasi lainnya. Hal ini terjadi ketika pengguna memiliki preferensi yang sama dengan pengguna lain yang tidak terduga.

Kekurangan CF:
1. Masalah cold start: Collaborative filtering menghadapi masalah cold start ketika sistem tidak memiliki informasi yang cukup tentang pengguna baru atau item baru. Dalam situasi ini, sulit untuk memberikan rekomendasi yang akurat karena tidak ada data riwayat yang cukup untuk digunakan dalam perhitungan rekomendasi.
2. Masalah skalabilitas: Ketika jumlah pengguna dan item sangat besar, perhitungan kesamaan dan penyimpanan data kolaboratif bisa menjadi rumit dan memakan waktu. Hal ini dapat mempengaruhi kinerja sistem rekomendasi dan memerlukan infrastruktur yang kuat untuk menangani volume data yang besar.
3. Efek "filter bubble": Collaborative filtering cenderung memperkuat preferensi yang ada dan membentuk apa yang disebut "filter bubble". Pengguna mungkin terpapar dengan rekomendasi yang serupa dan tidak cukup beragam, sehingga mengurangi variasi dan eksplorasi pengguna dalam menemukan hal-hal baru.

Penggunaan collaborative filtering yang efektif membutuhkan jumlah data yang signifikan dan keberadaan pengguna dan item yang serupa. Selain itu, perlu memperhatikan dan mengatasi masalah cold start dan efek "filter bubble" untuk memberikan rekomendasi yang berkualitas.

Hasil output dari CF ada di Gambar 5 sebagai berikut:
![CF rekomendasi](https://i.ibb.co/Rh9SS6K/CF.png)
Gambar 5. CF rekomendasi
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tf = TfidfVectorizer()
tf.fit(anime_ovrl['genre']) 
tf.get_feature_names_out()

tfidf_matrix = tf.fit_transform(anime_ovrl['genre'])
tfidf_matrix.shape

tfidf_matrix.todense()

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_ovrl['name'], columns=anime_ovrl['name'])
print('Shape:', cosine_sim_df.shape)
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""# **Evaluasi Content-based Filtering**"""

def anime_recommendations(name, similarity_data=cosine_sim_df, items=anime_ovrl[['name', 'genre']], k=5):
 
    index = similarity_data.loc[:,name].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop name agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(name, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

anime_ovrl[anime_ovrl.name.eq('Oni')]

anime_recommendations('Oni')

"""## **Collaborative filtering**"""

# Import library
import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.resto_embedding = layers.Embedding( # layer embeddings resto
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    resto_vector = self.resto_embedding(inputs[:, 1]) # memanggil layer embedding 3
    resto_bias = self.resto_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2) 
 
    x = dot_user_resto + user_bias + resto_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 50)
model.compile(loss=tf.keras.losses.BinaryCrossentropy(), 
              optimizer=keras.optimizers.Adam(learning_rate=0.0005),
              metrics=[tf.keras.metrics.MeanAbsoluteError()])

model.summary

history = model.fit(x=X_train, y=y_train,
                    batch_size=256, epochs=10, verbose=1, 
                    validation_data=(X_test, y_test))

"""# **Evaluasi Collaborative Filtering**

Dalam proyek ini penulis menggunakan metrik MAE(Mean Absolute Error) untuk *collaborative filtering* yang merupakan metrik evaluasi yang digunakan untuk mengukur kesalahan atau selisih antara nilai prediksi dan nilai aktual dalam konteks pemodelan atau peramalan. MAE mengukur kesalahan rata-rata secara absolut, yaitu selisih antara nilai prediksi dan nilai aktual tanpa mempertimbangkan arah atau signifikansi dari selisih tersebut. MAE menghitung nilai absolut dari setiap selisih dan kemudian mengambil rata-rata dari seluruh nilai absolut tersebut. MAE memiliki skala yang sama dengan variabel yang diukur, sehingga lebih mudah untuk diinterpretasikan daripada metrik evaluasi lainnya seperti Mean Squared Error (MSE) yang memiliki skala kuadrat.

![MAE](https://i.ibb.co/xzLxD31/mae.png)
Gambar 6. Rumus MAE
Rincian Sistematis rumus MAE pada Gambar 6 dapat dilihat sebagai berikut:

* MAE adalah Mean Absolute Error.
* n adalah jumlah sampel atau observasi.
* Σ adalah simbol sigma yang menunjukkan penjumlahan.
* yi adalah nilai aktual atau nilai yang diamati.
* xi adalah nilai prediksi atau nilai yang diprediksi.

Semakin rendah nilai MAE, semakin baik kualitas prediksi atau model yang digunakan. MAE digunakan dalam berbagai bidang seperti statistik, pemodelan data, dan machine learning untuk mengevaluasi kinerja model dalam memprediksi nilai atau variabel tertentu.

Lalu untuk model yang dihasilkan MAE terukur naik secara signifikan, hal ini dapat dilihat pada Gambar 7:

![Hasil MAE](https://i.ibb.co/bgGLrw6/hasil-MAE.png)
Gambar 7. Hasil MAE Menggunakan *collaborative filtering*

Pada Gambar 7 dapat dilihat kalau model *collaborative filtering* tidak cocok dengan data yang diujikan, hal ini dapat dipengaruhi beberapa faktor seperti:

1. Sparsitas data: Jika data kolaboratif memiliki banyak kekosongan atau kekurangan interaksi antara pengguna dan item, ini dapat menyebabkan nilai MAE yang tinggi. Ketika terdapat sedikit informasi atau interaksi yang tersedia untuk memperkirakan preferensi pengguna, prediksi yang dihasilkan cenderung kurang akurat.
2. Heterogenitas preferensi pengguna: Jika preferensi pengguna sangat beragam atau heterogen, ini dapat meningkatkan nilai MAE. Dalam situasi ini, sulit untuk menemukan pengguna yang memiliki preferensi yang mirip atau tetangga yang relevan untuk membuat prediksi yang akurat.
3. Data yang tidak lengkap atau tidak akurat: Jika data kolaboratif mengandung nilai yang hilang, data yang tidak lengkap, atau kesalahan dalam nilai, hal ini dapat mengganggu kemampuan model collaborative filtering untuk memberikan prediksi yang akurat. Data yang buruk atau tidak lengkap dapat menghasilkan kesalahan dalam perhitungan kesamaan atau estimasi preferensi.
4. Kurangnya informasi kontekstual: Collaborative filtering cenderung hanya mempertimbangkan interaksi antara pengguna dan item tanpa mempertimbangkan faktor kontekstual yang mungkin mempengaruhi preferensi pengguna. Faktor-faktor seperti waktu, lokasi, atau situasi yang dapat memengaruhi preferensi tidak dipertimbangkan secara eksplisit dalam metode collaborative filtering tradisional.
5. Kurangnya diversitas rekomendasi: Jika sistem collaborative filtering cenderung memberikan rekomendasi yang terlalu serupa atau kurang beragam, hal ini dapat menyebabkan nilai MAE yang tinggi. Pengguna mungkin memperoleh rekomendasi yang tidak cukup bervariasi atau tidak mampu menemukan preferensi baru yang berbeda dari preferensi yang sudah ada.
6. Masalah cold start: Masalah cold start muncul ketika sistem tidak memiliki informasi yang cukup tentang pengguna baru atau item baru. Dalam situasi ini, sulit untuk memberikan rekomendasi yang akurat karena kurangnya data riwayat atau interaksi.

Mengatasi faktor-faktor ini dapat membantu dalam mengurangi nilai MAE yang tinggi pada collaborative filtering. Beberapa pendekatan termasuk penggunaan teknik pengisian nilai yang hilang, peningkatan metode penghitungan kesamaan, penggunaan metode ensemble, dan memperkaya data dengan informasi kontekstual.
"""

plt.plot(history.history['mean_absolute_error'])
plt.plot(history.history['val_mean_absolute_error'])
plt.title('Hasil Pengujian Dengan Mean Absolute Error')
plt.ylabel('MAE')
plt.xlabel('EPOCH')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

user_id = 23423 # dapat diganti untuk memilih sampel user

movies_df = pd.read_csv('/content/kumpulan_dataset/Anime Dataset Rekomen/anime.csv')

watched_movies = df_train[df_train['user_id'] == user_id]

not_watched_movies = movies_df[~movies_df['anime_id'].isin(watched_movies['anime_id'].values)]['anime_id']
not_watched_movies = list(set(not_watched_movies).intersection(set(anime_to_anime_encoded.keys())))
not_watched_movies = [[anime_to_anime_encoded.get(x)] for x in not_watched_movies]

user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(([[user_encoder]] * len(not_watched_movies), not_watched_movies))

pred_ratings = model.predict(user_movie_array).flatten()
top_ratings_indices = pred_ratings.argsort()[-10:][::-1]
recommended_movie_ids = [anime_to_anime_encoded.get(not_watched_movies[x][0]) for x in top_ratings_indices]

print('Menampilkan Rekomendasi Anime Pada User: {}'.format(user_id))
print('===' * 9)
print('Anime dengan Rating Tertinggi yang diinput user')
print('----' * 8)
 
top_movies_user = (
    watched_movies.sort_values(
        by='rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

movies_df_rows = movies_df[movies_df['anime_id'].isin(top_movies_user)]
for row in movies_df_rows.itertuples():
    print(row.name, ':', row.genre)
    
print('----' * 8)
print('10 rekomendasi anime teratas')
print('----' * 8)
 
recommended_movies = movies_df[movies_df['anime_id'].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(row.name, ':', row.genre)